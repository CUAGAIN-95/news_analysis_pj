{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as wb\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스타이틀 크롤링을 위한 모듈 import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib import parse\n",
    "from Function import news_title_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코스피지수 크롤링\n",
    "#약 3년치 데이터를 크롤링해 옴\n",
    "start = datetime.datetime(2019, 8, 1)\n",
    "# 현재 날짜까지\n",
    "end = date.today()\n",
    "\n",
    "# ^KS11 : 코스피\n",
    "df_null = wb.DataReader(\"^KS11\",\"yahoo\",start,end)\n",
    "\n",
    "# 결측치 제거\n",
    "df = df_null.dropna()\n",
    "\n",
    "# Close와 Adj Close는 중복되는 columns인것을 확인 함\n",
    "# Adj Close열을 제거\n",
    "df.drop([\"Adj Close\"],axis=1, inplace=True)\n",
    "\n",
    "# 새로운 칼럼 생성\n",
    "# (Price : 당일 대비 다음날 주가가 상승했으면 1, 하락했으면 0 표시)\n",
    "df['Price'] = 0\n",
    "for i in range(len(df)-1):\n",
    "    if df['Close'][i] < df['Close'][i+1]:\n",
    "        df['Price'][i] = 1\n",
    "    else:\n",
    "        df['Price'][i] = 0\n",
    "\n",
    "# columns명을 알기 쉽게 한글로 변경\n",
    "df.columns = [\"최고가\", \"최저가\" , \"시작가\", \"종가\", \"거래량\" , \"등락\"]\n",
    "\n",
    "# 파일 저장\n",
    "df.to_csv('kospi_주가데이터.csv',encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등락 값이 \"0\"인 데이터와 \"1\"인 데이터를 나눠서 리스트화\n",
    "# 여기서 리스트화 하는 이유는 다음날 주가가 하락한 경우의 뉴스 타이틀과 상승한 뉴스타이틀을 크롤링해 분류하기 위함\n",
    "price_data = pd.read_csv('kospi_주가데이터.csv')\n",
    "\n",
    "#주가가 하락한 경우의 Date 데이터 리스트 date_0\n",
    "df_0 = price_data[price_data['등락']==0]['Date']\n",
    "date_0 = []\n",
    "for i in range(0, len(df_0)):\n",
    "    date_0.append(str(df_0.tolist()[i])[:10].replace('-', ''))\n",
    "\n",
    "#주가가 상승한 경우의 Date 데이터 리스트 date_1\n",
    "df_1 = price_data[price_data['등락']==1]['Date']\n",
    "date_1 = []\n",
    "for i in range(0, len(df_1)):\n",
    "    date_1.append(str(df_1.tolist()[i])[:10].replace('-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 타이틀 크롤링한 리스트 병합 후 csv파일로 변환\n",
    "def news_title_to_csv(site_name:str, date_0, date_1, base_url, news_block, news_title_block):\n",
    "    result_list = news_title_crawling(date_0, base_url, news_block, news_title_block)\n",
    "    title_df_0 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "    title_df_0['주가변동'] = 0\n",
    "\n",
    "    result_list = news_title_crawling(date_1, base_url, news_block, news_title_block)\n",
    "    title_df_1 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "    title_df_1['주가변동'] = 1\n",
    "\n",
    "    # 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "    title_df = pd.concat([title_df_0, title_df_1])\n",
    "    # 중복 데이터 삭제 \n",
    "    title_df.drop_duplicates(['뉴스제목'])\n",
    "    # 데이터프레임 저장 \n",
    "    title_df.to_csv(site_name + '_뉴스타이틀.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_title_to_csv(site_name:str, date_0, date_1, base_url, news_block, news_title_block)\n",
    "news_title_to_csv(\"팍스넷\", date_0, date_1,\n",
    "                base_url='http://www.paxnet.co.kr/news/much?newsSetId=4667&currentPageNo={1}&genDate={0}&objId=N4667',\n",
    "                news_block='ul.thumb-list li', \n",
    "                news_title_block='dl.text > dt')\n",
    "news_title_to_csv(\"네이버\", date_0, date_1,\n",
    "                base_url='https://finance.naver.com/news/mainnews.naver?date={0}&page={1}',\n",
    "                news_block='#contentarea_left > div.mainNewsList > ul > li > dl',\n",
    "                news_title_block='.articleSubject')\n",
    "news_title_to_csv(\"다음\", date_0, date_1,\n",
    "                base_url='https://news.daum.net/breakingnews/economic/stock?page={1}&regDate={0}',\n",
    "                news_block='ul.list_news2.list_allnews > li',\n",
    "                news_title_block='.link_txt')\n",
    "news_title_to_csv(\"네이트\", date_0, date_1,\n",
    "                base_url='https://news.nate.com/subsection?cate=eco06&mid=n0307&type=c&date={0}&page={1}',\n",
    "                news_block='div.postSubject > ul > li',\n",
    "                news_title_block='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스데이터 하나로 합치기 \n",
    "paxnet_data = pd.read_csv('팍스넷_뉴스타이틀.csv')\n",
    "naver_data = pd.read_csv('네이버_뉴스타이틀.csv')\n",
    "daum_data = pd.read_csv('다음_뉴스타이틀.csv')\n",
    "nate_data = pd.read_csv('네이트_뉴스타이틀.csv')\n",
    "all_title = pd.concat([naver_data, paxnet_data, daum_data, nate_data])\n",
    "all_title.to_csv('news_titles.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d1b1159014523827fe2501c4748f08e821ebb33f0d54f45f4bb6379e3df54b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
