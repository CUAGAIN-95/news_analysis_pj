{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as wb\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/jprthrkx73n0d9pl0mgr0kqm0000gn/T/ipykernel_22593/3761846299.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Price'][i] = 1\n",
      "/var/folders/3h/jprthrkx73n0d9pl0mgr0kqm0000gn/T/ipykernel_22593/3761846299.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Price'][i] = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>최고가</th>\n",
       "      <th>최저가</th>\n",
       "      <th>시작가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "      <th>등락</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-14</th>\n",
       "      <td>2499.429932</td>\n",
       "      <td>2470.600098</td>\n",
       "      <td>2485.179932</td>\n",
       "      <td>2474.649902</td>\n",
       "      <td>730900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15</th>\n",
       "      <td>2485.199951</td>\n",
       "      <td>2463.649902</td>\n",
       "      <td>2482.760010</td>\n",
       "      <td>2480.330078</td>\n",
       "      <td>617100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-16</th>\n",
       "      <td>2487.000000</td>\n",
       "      <td>2446.790039</td>\n",
       "      <td>2487.000000</td>\n",
       "      <td>2477.449951</td>\n",
       "      <td>657600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-17</th>\n",
       "      <td>2467.389893</td>\n",
       "      <td>2442.899902</td>\n",
       "      <td>2466.500000</td>\n",
       "      <td>2442.899902</td>\n",
       "      <td>914519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    최고가          최저가          시작가           종가     거래량  등락\n",
       "Date                                                                      \n",
       "2022-11-14  2499.429932  2470.600098  2485.179932  2474.649902  730900   1\n",
       "2022-11-15  2485.199951  2463.649902  2482.760010  2480.330078  617100   0\n",
       "2022-11-16  2487.000000  2446.790039  2487.000000  2477.449951  657600   0\n",
       "2022-11-17  2467.389893  2442.899902  2466.500000  2442.899902  914519   0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코스피지수 크롤링\n",
    "#약 3년치 데이터를 크롤링해 옴\n",
    "start = datetime.datetime(2019, 8, 1)\n",
    "# 현재 날짜까지\n",
    "end = date.today()\n",
    "\n",
    "# ^KS11 : 코스피\n",
    "df_null = wb.DataReader(\"^KS11\",\"yahoo\",start,end)\n",
    "\n",
    "# 결측치 제거\n",
    "df = df_null.dropna()\n",
    "\n",
    "# Close와 Adj Close는 중복되는 columns인것을 확인 함\n",
    "# Adj Close열을 제거\n",
    "df.drop([\"Adj Close\"],axis=1, inplace=True)\n",
    "\n",
    "# 새로운 칼럼 생성\n",
    "# (Price : 당일 대비 다음날 주가가 상승했으면 1, 하락했으면 0 표시)\n",
    "df['Price'] = 0\n",
    "for i in range(len(df)-1):\n",
    "    if df['Close'][i] < df['Close'][i+1]:\n",
    "        df['Price'][i] = 1\n",
    "    else:\n",
    "        df['Price'][i] = 0\n",
    "\n",
    "# columns명을 알기 쉽게 한글로 변경\n",
    "df.columns = [\"최고가\", \"최저가\" , \"시작가\", \"종가\", \"거래량\" , \"등락\"]\n",
    "\n",
    "# 파일 저장\n",
    "df.to_csv('kospi_주가데이터.csv',encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20221114']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 등락 값이 \"0\"인 데이터와 \"1\"인 데이터를 나눠서 리스트화\n",
    "# 여기서 리스트화 하는 이유는 다음날 주가가 하락한 경우의 뉴스 타이틀과 상승한 뉴스타이틀을 크롤링해 분류하기 위함\n",
    "price_data = pd.read_csv('kospi_주가데이터.csv')\n",
    "\n",
    "#주가가 하락한 경우의 Date 데이터 리스트 date_0\n",
    "df_0 = price_data[price_data['등락']==0]['Date']\n",
    "date_0 = []\n",
    "for i in range(0,len(df_0)):\n",
    "    date_0.append(str(df_0.tolist()[i])[:10].replace('-',''))\n",
    "\n",
    "#주가가 상승한 경우의 Date 데이터 리스트 date_1\n",
    "df_1 = price_data[price_data['등락']==1]['Date']\n",
    "date_1 = []\n",
    "for i in range(0,len(df_1)):\n",
    "    date_1.append(str(df_1.tolist()[i])[:10].replace('-',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스타이틀 크롤링을 위한 모듈 import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스타이틀 크롤링한 뒤 제목 리스틑 리턴\n",
    "def news_title_crawling(dates, base_url, news_block, news_title_block) -> list:\n",
    "    result_list = []\n",
    "    error_cnt = 0\n",
    "    headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "    }\n",
    "    for date in dates:\n",
    "        for page in range(1, 3):\n",
    "            url = base_url.format(date, page)\n",
    "            res = requests.get(url, headers=headers)\n",
    "            if res.status_code == 200:\n",
    "                soup = BeautifulSoup(res.text)\n",
    "                title_list = soup.select(news_block)\n",
    "                for title in title_list:\n",
    "                    try:\n",
    "                        news_title = title.select_one(news_title_block).text.strip()\n",
    "                        find_1 = news_title.find(\"[\")\n",
    "                        find_2 = news_title.find(\"]\")\n",
    "                        if find_1 != -1:\n",
    "                            slice_news_title = news_title[find_1:find_2+1]\n",
    "                            news_title.strip(slice_news_title)\n",
    "                            result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "                        elif find_1 == -1:\n",
    "                            result_list.append([news_title])\n",
    "                    except:\n",
    "                        error_cnt += 1\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 타이틀 크롤링한 리스트 병합 후 csv파일로 변환\n",
    "def news_title_to_csv(site_name:str, date_0, date_1, base_url, news_block, news_title_block):\n",
    "    result_list = news_title_crawling(date_0, base_url, news_block, news_title_block)\n",
    "    title_df_0 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "    title_df_0['주가변동'] = 0\n",
    "\n",
    "    result_list = news_title_crawling(date_1, base_url, news_block, news_title_block)\n",
    "    title_df_1 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "    title_df_1['주가변동'] = 1\n",
    "\n",
    "    # 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "    title_df = pd.concat([title_df_0, title_df_1])\n",
    "    # 중복 데이터 삭제 \n",
    "    title_df.drop_duplicates(['뉴스제목'])\n",
    "    # 데이터프레임 저장 \n",
    "    title_df.to_csv(site_name + '_뉴스타이틀.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_title_to_csv(site_name:str, date_0, date_1, base_url, news_block, news_title_block)\n",
    "news_title_to_csv(\"팍스넷\", date_0, date_1,\n",
    "                base_url='http://www.paxnet.co.kr/news/much?newsSetId=4667&currentPageNo={1}&genDate={0}&objId=N4667',\n",
    "                news_block='ul.thumb-list li', \n",
    "                news_title_block='dl.text > dt')\n",
    "news_title_to_csv(\"네이버\", date_0, date_1,\n",
    "                base_url='https://finance.naver.com/news/mainnews.naver?date={0}&page={1}',\n",
    "                news_block='#contentarea_left > div.mainNewsList > ul > li > dl',\n",
    "                news_title_block='.articleSubject')\n",
    "news_title_to_csv(\"다음\", date_0, date_1,\n",
    "                base_url='https://news.daum.net/breakingnews/economic/stock?page={1}&regDate={0}',\n",
    "                news_block='ul.list_news2.list_allnews > li',\n",
    "                news_title_block='.link_txt')\n",
    "news_title_to_csv(\"네이트\", date_0, date_1,\n",
    "                base_url='https://news.nate.com/subsection?cate=eco06&mid=n0307&type=c&date={0}&page={1}',\n",
    "                news_block='div.postSubject > ul > li',\n",
    "                news_title_block='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스데이터 하나로 합치기 \n",
    "paxnet_data = pd.read_csv('팍스넷_뉴스타이틀.csv')\n",
    "naver_data = pd.read_csv('네이버_뉴스타이틀.csv')\n",
    "daum_data = pd.read_csv('다음_뉴스타이틀.csv')\n",
    "nate_data = pd.read_csv('네이트_뉴스타이틀.csv')\n",
    "all_title = pd.concat([naver_data, paxnet_data, daum_data, nate_data])\n",
    "all_title.to_csv('news_titles.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 팍스넷 크롤링 함수 paxnet_news_title\n",
    "# result_list = []\n",
    "# error_cnt = 0\n",
    "\n",
    "# def paxnet_news_title(dates):\n",
    "#     base_url = 'http://www.paxnet.co.kr/news/much?newsSetId=4667&currentPageNo={}&genDate={}&objId=N4667'\n",
    "#     headers = {\n",
    "#         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "#     }\n",
    "    \n",
    "#     for date in dates:\n",
    "#         for page in range(1, 3):\n",
    "#             url = base_url.format(page, date)\n",
    "#             res = requests.get(url, headers=headers)\n",
    "#             if res.status_code == 200:\n",
    "#                 soup = BeautifulSoup(res.text)\n",
    "#                 title_list = soup.select('ul.thumb-list li')\n",
    "#                 for title in title_list:\n",
    "#                     try:\n",
    "#                         news_title = title.select_one('dl.text > dt').text.strip()\n",
    "#                         #\"[문자]\" 가 붙은 제거\n",
    "#                         #find 함수는 문자열안에 지정한 문자가 있을경우 그 위치(index)를 int로 반환해준다.\n",
    "#                         #지정한 문자를 찾지 못할 경우 -1 을 반환 \n",
    "#                         find_1=news_title.find(\"[\")\n",
    "#                         find_2=news_title.find(\"]\")\n",
    "\n",
    "#                         # find함수의 결과가 -1가 아닌 경우(문자열안에 \"[문자]\"가 있다.)\n",
    "#                         # 그 위치를 찾아 제거하고 result_list에 추가\n",
    "#                         if find_1 != -1:\n",
    "#                             slice_news_title=news_title[find_1:find_2+1]\n",
    "#                             result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "#                         #find함수의 결과가 -1 일경우(문자열안에 \"[문자]\"가 없다.)\n",
    "#                         #바로 result_list에 추가\n",
    "#                         elif find_1 == -1:\n",
    "#                             result_list.append([news_title])\n",
    "#                     except:\n",
    "#                         error_cnt += 1\n",
    "# paxnet_news_title(date_0)\n",
    "# title_df_0 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_0['주가변동'] = 0\n",
    "# result_list = []\n",
    "\n",
    "# paxnet_news_title(date_1)\n",
    "# title_df_1 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_1['주가변동'] = 1\n",
    "# result_list = []\n",
    "\n",
    "\n",
    "\n",
    "# #팍스넷 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "# paxnet_title_df = pd.concat([title_df_0, title_df_1])\n",
    "# #중복 데이터 삭제 \n",
    "# paxnet_title_df.drop_duplicates(['뉴스제목'])\n",
    "# #데이터프레임 저장 \n",
    "# paxnet_title_df.to_csv('팍스넷_뉴스타이틀.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 네이버 크롤링 함수 naver_news_title\n",
    "# result_list = []\n",
    "# error_cnt = 0\n",
    "# def naver_news_title(dates):\n",
    "#     base_url = 'https://finance.naver.com/news/mainnews.naver?date={}&page={}'\n",
    "#     headers = {\n",
    "#         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "#     }\n",
    "#     for date in dates:\n",
    "#         for page in range(1, 3):\n",
    "#             url = base_url.format(date,page)\n",
    "#             res = requests.get(url, headers=headers)\n",
    "#             if res.status_code == 200:\n",
    "#                 soup = BeautifulSoup(res.text)\n",
    "#                 title_list = soup.select('#contentarea_left > div.mainNewsList > ul > li > dl')\n",
    "#                 for title in title_list:\n",
    "#                     try:\n",
    "#                         news_title = title.select_one('.articleSubject').text.strip()\n",
    "#                         find_1=news_title.find(\"[\")\n",
    "#                         find_2=news_title.find(\"]\")\n",
    "#                         if find_1 != -1:\n",
    "#                             slice_news_title=news_title[find_1:find_2+1]\n",
    "#                             news_title.strip(slice_news_title)\n",
    "#                             result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "#                         elif find_1 == -1:\n",
    "#                             result_list.append([news_title])\n",
    "#                     except:\n",
    "#                         error_cnt += 1\n",
    "\n",
    "# naver_news_title(date_0)\n",
    "# title_df_2 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_2['주가변동'] = 0\n",
    "# result_list = []\n",
    "\n",
    "# naver_news_title(date_1)\n",
    "# title_df_3 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_3['주가변동'] = 1\n",
    "# result_list = []\n",
    "\n",
    "# #네이버 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "# naver_title_df = pd.concat([title_df_2, title_df_3])\n",
    "# #중복 데이터 삭제 \n",
    "# naver_title_df.drop_duplicates(['뉴스제목'])\n",
    "# naver_title_df.to_csv('네이버_뉴스타이틀.csv', index=False, encoding='utf-8-sig')\n",
    "# naver_title_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다음 크롤링 함수 daum_news_title\n",
    "# result_list = []\n",
    "# error_cnt = 0\n",
    "# def daum_news_title(dates):\n",
    "#     base_url = \"https://news.daum.net/breakingnews/economic/stock?page={1}&regDate={0}\"\n",
    "#     headers = {\n",
    "#         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "#     }\n",
    "#     for date in dates:\n",
    "#         for page in range(1, 3):\n",
    "#             url = base_url.format(date, page)\n",
    "#             res = requests.get(url, headers=headers)\n",
    "#             if res.status_code == 200:\n",
    "#                 soup = BeautifulSoup(res.text)\n",
    "#                 title_list = soup.select('ul.list_news2.list_allnews > li')\n",
    "#                 for title in title_list:\n",
    "#                     try:\n",
    "#                         news_title = title.select_one('.link_txt').text.strip()\n",
    "#                         find_1=news_title.find(\"[\")\n",
    "#                         find_2=news_title.find(\"]\")\n",
    "#                         if find_1 != -1:\n",
    "#                             slice_news_title=news_title[find_1:find_2+1]\n",
    "#                             news_title.strip(slice_news_title)\n",
    "#                             result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "#                         elif find_1 == -1:\n",
    "#                             result_list.append([news_title])\n",
    "#                     except:\n",
    "#                         error_cnt += 1\n",
    "\n",
    "# daum_news_title(date_0)\n",
    "# title_df_4 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_4['주가변동'] = 0\n",
    "# result_list = []\n",
    "\n",
    "# daum_news_title(date_1)\n",
    "# title_df_5 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_5['주가변동'] = 1\n",
    "# result_list = []\n",
    "\n",
    "# #다음 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "# daum_title_df = pd.concat([title_df_4, title_df_5])\n",
    "# #중복 데이터 삭제 \n",
    "# daum_title_df.drop_duplicates(['뉴스제목'])\n",
    "# daum_title_df.to_csv('다음_뉴스타이틀.csv', index=False, encoding='utf-8-sig')\n",
    "# daum_title_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 네이트 크롤링 함수 nate_news_title\n",
    "# result_list = []\n",
    "# error_cnt = 0\n",
    "# def nate_news_title(dates):\n",
    "#     base_url = \"https://news.nate.com/subsection?cate=eco04&mid=n0304&type=t&date={0}\"\n",
    "#     headers = {\n",
    "#         'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "#     }\n",
    "#     for date in dates:\n",
    "#         url = base_url.format(date)\n",
    "#         res = requests.get(url, headers=headers)\n",
    "#         if res.status_code == 200:\n",
    "#             soup = BeautifulSoup(res.text)\n",
    "#             title_list = soup.select('div.postSubject > ul > li')\n",
    "#             for title in title_list:\n",
    "#                 try:\n",
    "#                     news_title = title.select_one('a').text.strip()\n",
    "#                     find_1=news_title.find(\"[\")\n",
    "#                     find_2=news_title.find(\"]\")\n",
    "#                     if find_1 != -1:\n",
    "#                         slice_news_title=news_title[find_1:find_2+1]\n",
    "#                         news_title.strip(slice_news_title)\n",
    "#                         result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "#                     elif find_1 == -1:\n",
    "#                         result_list.append([news_title])\n",
    "#                 except:\n",
    "#                     error_cnt += 1\n",
    "\n",
    "# nate_news_title(date_0)\n",
    "# title_df_6 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_6['주가변동'] = 0\n",
    "# result_list = []\n",
    "\n",
    "# nate_news_title(date_1)\n",
    "# title_df_7 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "# title_df_7['주가변동'] = 1\n",
    "# result_list = []\n",
    "\n",
    "# #네이트 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "# nate_title_df = pd.concat([title_df_6, title_df_7])\n",
    "# #중복 데이터 삭제 \n",
    "# nate_title_df.drop_duplicates(['뉴스제목'])\n",
    "# nate_title_df.to_csv('네이트_뉴스타이틀.csv', index=False, encoding='utf-8-sig')\n",
    "# nate_title_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
